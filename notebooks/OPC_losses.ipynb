{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "id": "laviAkh3kE5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install lpips"
      ],
      "metadata": {
        "id": "rftnlUdEh-5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader, Dataset"
      ],
      "metadata": {
        "id": "osDpuDO2Zpta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMa3gkPbbLpC",
        "outputId": "19e73c64-9932-4e94-83b6-f7a899c6be74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1slbiyvE5gYnaYMkgD42Iwx1u_NhpAv_A' -O loss_selection.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5lJwALz94pW",
        "outputId": "fc10615d-ca78-4383-8960-d4d5f5dc49fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-16 09:02:48--  https://docs.google.com/uc?export=download&id=1slbiyvE5gYnaYMkgD42Iwx1u_NhpAv_A\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.170.139, 142.251.170.102, 142.251.170.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.170.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1slbiyvE5gYnaYMkgD42Iwx1u_NhpAv_A&export=download [following]\n",
            "--2025-02-16 09:02:49--  https://drive.usercontent.google.com/download?id=1slbiyvE5gYnaYMkgD42Iwx1u_NhpAv_A&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 108.177.125.132, 2404:6800:4008:c01::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|108.177.125.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2813204 (2.7M) [application/octet-stream]\n",
            "Saving to: ‘loss_selection.zip’\n",
            "\n",
            "loss_selection.zip  100%[===================>]   2.68M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-02-16 09:02:54 (101 MB/s) - ‘loss_selection.zip’ saved [2813204/2813204]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip loss_selection.zip\n",
        "!rm -r /content/__MACOSX"
      ],
      "metadata": {
        "id": "F4yHxrnOba2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BinarizeTransform:\n",
        "    def __init__(self, threshold=0.5):\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        # Assuming the input tensor is of shape (1, 1, 1024, 1024)\n",
        "        # Convert the tensor to a NumPy array\n",
        "        image_array = tensor.squeeze().numpy()  # Shape will be (1024, 1024)\n",
        "\n",
        "        # Binarize the image\n",
        "        binarized_image = (image_array > self.threshold).astype(np.float32)  # Binary (0 or 1)\n",
        "\n",
        "        # Convert back to tensor and maintain shape (1, 1, 1024, 1024)\n",
        "        return torch.from_numpy(binarized_image).unsqueeze(0)  # Shape will be (1, 1, 1024, 1024)\n",
        "\n",
        "class EvalDataset(Dataset):\n",
        "    def __init__(self, target_dir, mask_dir, transform=None):\n",
        "        self.target_dir = target_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.transform = transform\n",
        "        self.images = os.listdir(target_dir)\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    def __getitem__(self, index):\n",
        "        target_path = os.path.join(self.target_dir, self.images[index])\n",
        "        mask_path = os.path.join(self.mask_dir, self.images[index])\n",
        "        target = Image.open(target_path)\n",
        "        mask = Image.open(mask_path)\n",
        "\n",
        "        if self.transform:\n",
        "            target = self.transform(target)\n",
        "            mask = self.transform(mask)\n",
        "        return target, mask"
      ],
      "metadata": {
        "id": "fv6V1Lh-ZtRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_PATH = '/content/loss_selection/gt'\n",
        "MASK_PATH = '/content/loss_selection/pred'\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "TRANSFORM = transforms.Compose([transforms.Resize((1024,1024)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Grayscale(),\n",
        "                                BinarizeTransform(threshold=0.5)])\n",
        "\n",
        "EVAL_DATASET = EvalDataset(TARGET_PATH, MASK_PATH, transform = TRANSFORM)\n",
        "EVAL_LOADER = DataLoader(EVAL_DATASET, batch_size = BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "mU4kGFy1Z_L-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gt, pred = next(iter(EVAL_LOADER))\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10,10))\n",
        "axes[0].imshow(gt.squeeze(), cmap='gray')\n",
        "axes[0].set_title('Ground-truth correction')\n",
        "axes[0].axis('off')\n",
        "\n",
        "axes[1].imshow(pred.squeeze(), cmap='gray')\n",
        "axes[1].set_title('Predicted correction')\n",
        "axes[1].axis('off')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "aOxuBEAqbm6Z",
        "outputId": "6e67af3b-b0b4-4743-e401-f9460c7c1868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(-0.5, 1023.5, 1023.5, -0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize difference"
      ],
      "metadata": {
        "id": "qos31MT-Dud1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.imshow(torch.abs(pred - gt).squeeze().numpy(), vmin=0, vmax=1)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "cf6ip6OpDwzs",
        "outputId": "666d68d7-ea39-4ba7-bb08-e465655895c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x1000 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IoU loss"
      ],
      "metadata": {
        "id": "L6ggD1-PeOdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IouLoss(nn.Module):\n",
        "  def __init__(self, weight=1.0, eps=1e-6):\n",
        "    super(IouLoss, self).__init__()\n",
        "    self.weight = weight\n",
        "    self.eps = eps\n",
        "  def forward(self, pred, target):\n",
        "    intersection = (pred * target).sum(dim=(2, 3))\n",
        "    union = (pred + target).sum(dim=(2, 3)) - intersection\n",
        "    iou = (intersection + self.eps) / (union + self.eps)\n",
        "    return (1 - iou.mean()) * self.weight\n",
        "\n",
        "iou_loss = IouLoss()\n",
        "print(f'IoU loss value:{iou_loss(gt, pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncU3g4O3eNjv",
        "outputId": "37c7006b-7c74-4882-8afb-411226deefe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IoU loss value:0.010557770729064941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAE Loss"
      ],
      "metadata": {
        "id": "_vakwOrreQ8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l1_loss = torch.nn.L1Loss()\n",
        "print(f'L1_loss value: {l1_loss(gt, pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1ZicOyLfEjD",
        "outputId": "fd3a71be-2653-47b5-c5f6-3f5cf5cc8142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1_loss value: 0.0010251998901367188\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.image import TotalVariation\n",
        "\n",
        "l1_loss_sum = torch.nn.L1Loss(reduction='sum')\n",
        "l1_loss_mean = torch.nn.L1Loss()\n",
        "\n",
        "l1_score_sum = l1_loss_sum(gt, pred)\n",
        "print(f'L1 sum:{l1_score_sum}')\n",
        "\n",
        "l1_score_mean = l1_loss_mean(gt, pred)\n",
        "print(f'L1 mean:{l1_score_mean}')\n",
        "\n",
        "print(f'Custom calculation #1: {((torch.abs(gt - pred)).sum() / (1024 * 1024))}')\n",
        "print(f'Summ: {torch.abs(gt - pred).sum()}')\n",
        "\n",
        "# normalize by union\n",
        "print(f'Custom calculation #2: {((torch.abs(gt - pred)).sum() / ((torch.abs(gt + pred)).sum()))}')\n",
        "# normalize by intersection\n",
        "print(f'Custom calculation #3: {((torch.abs(gt - pred)).sum() / ((torch.abs(gt * pred)).sum()))}')\n",
        "# normalize by gt area\n",
        "print(f'Custom calculation #4: {(torch.abs(gt - pred)).sum() / ((gt.sum()))}')\n",
        "\n",
        "tv = TotalVariation()\n",
        "print(f'Total Variation for gt: {tv(gt)}')\n",
        "print(f'Total Variation for pred: {tv(pred)}')\n",
        "print(f'Relative: {torch.abs((tv(gt) - tv(pred))) / tv(gt) }')"
      ],
      "metadata": {
        "id": "bc94YMnMaXRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Structural similarity index (SSIM) loss\n",
        "\n",
        "**What It Measures:** SSIM evaluates the perceived quality of an image by comparing luminance, contrast, and structure between the generated and ground-truth images.\n",
        "**Why Use It:** SSIM is better at capturing perceptual differences than pixel-wise metrics like IoU or Pixel Accuracy. It is particularly useful for evaluating the structural similarity of images."
      ],
      "metadata": {
        "id": "uk0kah1wfmQ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fol-5lxYjyIS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9386f5bd-98b9-4428-ae3d-f5708be129f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SSIM_loss value: 0.9926246404647827\n"
          ]
        }
      ],
      "source": [
        "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM\n",
        "\n",
        "ssim = SSIM(data_range=1.0)  # Assuming pixel values are in [0, 1]\n",
        "ssim_score = ssim(pred, gt)\n",
        "print(f'SSIM_loss value: {ssim_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge Loss\n",
        "\n",
        "**What It Measures:** These metrics evaluate how well the edges in the generated image match those in the ground-truth image.\n",
        "\n",
        "**Why Use It:** For OPC tasks, preserving sharp edges is critical. Edge preservation metrics can help assess the quality of edge reconstruction\n",
        "\n",
        "**Implementation**:\n",
        "\n",
        "\n",
        "*   Use Sobel or Canny edge detector to extract edges from both generated and ground-truth images\n",
        "*   Compute L1-difference between derived edges\n",
        "*   Two implementations are provided: raw difference between edges and difference normalized by the area of the target itself\n",
        "\n"
      ],
      "metadata": {
        "id": "7d2GAS_fgfBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def sobel_edge_detector(image):\n",
        "    kernel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(image.device)\n",
        "    kernel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], dtype=torch.float32).view(1, 1, 3, 3).to(image.device)\n",
        "    grad_x = F.conv2d(image, kernel_x, padding=1)\n",
        "    grad_y = F.conv2d(image, kernel_y, padding=1)\n",
        "    return torch.sqrt(grad_x ** 2 + grad_y ** 2)\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "edges_pred = sobel_edge_detector(pred)\n",
        "edges_target = sobel_edge_detector(gt)\n",
        "edge_loss_1 = F.l1_loss(edges_pred, edges_target)\n",
        "\n",
        "edge_loss_2 = F.l1_loss(edges_pred, edges_target, reduction='sum')\n",
        "print(f'Edge_loss #1: {edge_loss_1}')\n",
        "print(f'Edge_loss #2: {edge_loss_2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smwDXvsEgLN6",
        "outputId": "7adc25cf-a245-479f-cae5-85b0f98cc21e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Edge_loss #1: 0.008262952789664268\n",
            "Edge_loss #2: 8664.333984375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ContourLoss(nn.Module):\n",
        "  def __init__(self, weight=1.0, device='cpu'):\n",
        "    super(ContourLoss, self).__init__()\n",
        "    self.weight = weight\n",
        "    self.device = device\n",
        "    self.sobel_kernel_x = torch.tensor([[1, 0, -1], [2, 0, -2], [1,0,-1]], dtype = torch.float32, device = device).view(1,1,3,3)\n",
        "    self.sobel_kernel_y = torch.tensor([[1,2,1], [0,0,0], [-1, -2, -1]], dtype = torch.float32, device = device).view(1,1,3,3)\n",
        "  def forward(self, pred, target):\n",
        "    target_edge_x = torch.nn.functional.conv2d(target, self.sobel_kernel_x, padding = 1)\n",
        "    target_edge_y = torch.nn.functional.conv2d(target, self.sobel_kernel_y, padding = 1)\n",
        "    target_edge = torch.sqrt(target_edge_x ** 2 + target_edge_y ** 2 + 1e-6)\n",
        "    pred_edge_x = torch.nn.functional.conv2d(pred, self.sobel_kernel_x, padding = 1)\n",
        "    pred_edge_y = torch.nn.functional.conv2d(pred, self.sobel_kernel_y, padding = 1)\n",
        "    pred_edge = torch.sqrt(pred_edge_x ** 2 + pred_edge_y ** 2 + 1e-6)\n",
        "    # calculate difference for every predicted and target correction in a batch\n",
        "    loss_per_image = torch.abs(pred_edge - target_edge).sum(dim=(1,2,3)) # (N, ), sum over height, width and channels\n",
        "    # calculate the contour area for every target correction in a batch\n",
        "    target_edge_sum = target_edge.sum(dim=(1,2,3)) + 1e-6 # (N, ) sum over height, width, and prevent division by zero\n",
        "    # normalise the difference by the contour area of target correction\n",
        "    mean_loss = (loss_per_image / target_edge_sum) * self.weight\n",
        "    # return mean value over a batch\n",
        "    return mean_loss.mean()\n",
        "contour_loss = ContourLoss()\n",
        "print(f'Contour Loss value: {contour_loss(pred, gt)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "813OF3YuM-eg",
        "outputId": "f81cdc40-01a3-4967-9d4f-6cc19ffc47fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contour Loss value: 0.2712148129940033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptual Loss\n",
        "\n",
        "**What It Measures:** LPIPS (Learned Perceptual Image Patch Similarity) uses a pre-trained neural network (e.g., VGG) to compare the perceptual similarity between images\n",
        "\n",
        "**Why Use It:** LPIPS captures high-level features and is more aligned with human perception than pixel-wise metrics"
      ],
      "metadata": {
        "id": "zyrbUgP2h6tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import lpips\n",
        "\n",
        "lpips_loss_vgg = lpips.LPIPS(net='vgg')  # Use 'alex' or 'vgg'\n",
        "lpips_loss_alex = lpips.LPIPS(net='alex')\n",
        "\n",
        "lpips_score_vgg = lpips_loss_vgg(pred, gt)\n",
        "lpips_score_alex = lpips_loss_alex(pred, gt)\n",
        "\n",
        "print(f'Perceptual Loss value (VGG): {lpips_score_vgg}')\n",
        "print(f'Perceptual Loss value (AlexNet): {lpips_score_alex}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdhE41ElhON5",
        "outputId": "6f1477d0-f081-4dbc-8a50-f50057f82c21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/vgg.pth\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "Perceptual Loss value (VGG): tensor([[[[0.0153]]]], grad_fn=<AddBackward0>)\n",
            "Perceptual Loss value (AlexNet): tensor([[[[0.0135]]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Frechet Inception distance\n",
        "\n",
        "**What It Measures:** FID evaluates the quality of generated images by comparing the distribution of features extracted from a pre-trained Inception network.\n",
        "\n",
        "**Why Use It:** FID is widely used in generative models to assess the realism and diversity of generated images\n",
        "\n",
        "*thrown an error, seems to be working with RGB images*"
      ],
      "metadata": {
        "id": "vRBgbhkbj0L4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-fidelity"
      ],
      "metadata": {
        "id": "lBGpEYKdkHLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "\n",
        "fid = FrechetInceptionDistance(feature=2048)  # Use feature=768 for Inception v3\n",
        "fid.update(torch.tensor(pred, dtype=torch.uint8), real=False)\n",
        "fid.update(torch.tensor(gt, dtype=torch.uint8), real=True)\n",
        "fid_score = fid.compute()\n",
        "print(f'Frechet loss value:{fid_score}')"
      ],
      "metadata": {
        "id": "SDJvGuqDjGkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dice coefficient\n",
        "\n",
        "**What It Measures:** The Dice coefficient is similar to IoU but is more sensitive to overlapping regions.\n",
        "\n",
        "**Why Use It:** It is commonly used in medical imaging and segmentation tasks to evaluate overlap between predicted and ground-truth masks"
      ],
      "metadata": {
        "id": "V9HeRV6_mq_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_coefficient(pred, target, eps=1e-6):\n",
        "    intersection = (pred * target).sum()\n",
        "    union = pred.sum() + target.sum()\n",
        "    return (2 * intersection + eps) / (union + eps)\n",
        "\n",
        "dice_score = dice_coefficient(pred, gt)\n",
        "print(f'Dice score: {dice_score}')\n",
        "print(f'Dice loss: {1 - dice_score}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm6Tnt24kCzC",
        "outputId": "8a92e1b3-0204-4918-b645-01cf51db44f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dice score: 0.9946931004524231\n",
            "Dice loss: 0.005306899547576904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fourier loss\n",
        "\n",
        "Seems to be the best solution!!!\n",
        "\n",
        "> Since your enhancer network is struggling with small pixel-level distortions and edge artifacts, a frequency-domain approach might help by removing high-frequency noise and preserving the structure. Here’s how you can integrate Fourier Transform-based loss functions into your training.\n",
        "\n",
        "🔹 Why Use Frequency Domain Processing?\n",
        "\n",
        "1️⃣ Edge distortions often appear as high-frequency components → Can be suppressed in the frequency domain\n",
        "\n",
        "2️⃣ Important structural features (shapes, contours) have low-frequency components → Should be preserved\n",
        "\n",
        "3️⃣ Pixel-wise losses (L1, L2) focus on spatial similarity, but frequency loss captures structural consistency\n",
        "\n",
        "\n",
        "\n",
        "🔹 How to Convert an Image to the Frequency Domain?\n",
        "\n",
        "The Fourier Transform (FFT) decomposes an image into frequency components:\n",
        "\n",
        "\t•\tLow frequencies → Large-scale structures (overall shape)\n",
        "\t•\tHigh frequencies → Fine details (sharp edges, noise, artifacts)\n"
      ],
      "metadata": {
        "id": "l4NaNxpmkGxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_frequency_domain(image):\n",
        "  '''\n",
        "  obtain the magnitude spectrum which tells us the strength of different\n",
        "  frequency components\n",
        "  '''\n",
        "  fft = torch.fft.fft2(image) # Compute 2d Fourier transform\n",
        "  fft_shift = torch.fft.fftshift(fft) # Shift zero frequency to center\n",
        "  magnitude = torch.abs(fft_shift) # Get magnitude spectrum\n",
        "\n",
        "  return magnitude\n",
        "\n",
        "def fourier_loss(pred, target):\n",
        "  pred_freq = to_frequency_domain(pred)\n",
        "  target_freq = to_frequency_domain(target)\n",
        "\n",
        "  return F.l1_loss(pred_freq, target_freq)\n",
        "\n",
        "print(f'Fourier loss: {fourier_loss(pred, gt)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f36cBl4LkJKo",
        "outputId": "b9f4c575-7849-44c4-8194-84d8c3c774b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fourier loss: 10.269524574279785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def high_pass_filter(freq_image, threshold=20):\n",
        "  \"\"\"\n",
        "  Zero out  low-frequency components below the threshold\n",
        "  \"\"\"\n",
        "  _, _, h, w = freq_image.shape\n",
        "  center_h ,center_w = h // 2, w // 2\n",
        "  mask = torch.ones_like(freq_image)\n",
        "  mask[:, :, center_h-threshold:center_h+threshold, center_w-threshold:center_w+threshold] = 0\n",
        "  return freq_image * mask\n",
        "\n",
        "def high_frequency_loss(pred, target):\n",
        "  pred_freq = to_frequency_domain(pred)\n",
        "  target_freq = to_frequency_domain(target)\n",
        "\n",
        "  pred_high = high_pass_filter(pred_freq)\n",
        "  target_high = high_pass_filter(target_freq)\n",
        "\n",
        "  return F.l1_loss(pred_high, target_high)\n",
        "print(f'High frequency loss: {high_frequency_loss(pred, gt)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30IiGZyMlj8v",
        "outputId": "76c83882-d6b5-4f41-f422-61f4bc952748"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "High frequency loss: 10.149885177612305\n"
          ]
        }
      ]
    }
  ]
}